{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 torchtext==0.16.0 numpy==1.24.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UNqbQ-BMy9vs",
        "outputId": "4929f4de-f393-470c-bae3-7daf1107dba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.1.0\n",
            "  Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchtext==0.16.0\n",
            "  Downloading torchtext-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting numpy==1.24.2\n",
            "  Downloading numpy-1.24.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.16.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.16.0) (2.32.3)\n",
            "Collecting torchdata==0.7.0 (from torchtext==0.16.0)\n",
            "  Downloading torchdata-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.5.82)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.16.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.16.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.16.0) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.16.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.2 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.2 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.2 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.2 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.2 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.24.2 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.2 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.2 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 torchdata-0.7.0 torchtext-0.16.0 triton-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "7bcd4c6a7dac46fcb595cd65b5148147"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKJGfZDLylPP",
        "outputId": "6d36a311-cd3a-40b3-e492-61fec097cbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Read QA dataset (JSON format)\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Read QA dataset (JSON format)\n",
        "def read_qa_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw = json.load(f)\n",
        "    raw_data = []\n",
        "    for item in raw:\n",
        "        question = item[\"question\"].strip()\n",
        "        context = item[\"context\"].strip()\n",
        "        answer = item[\"answers\"][\"text\"][0].strip()\n",
        "        input_text = question + \" [SEP] \" + context\n",
        "        raw_data.append((input_text, answer))\n",
        "    return raw_data\n",
        "\n",
        "# Splitting the dataset\n",
        "def split_dataset(data, train_split=0.7, val_split=0.15, test_split=0.15):\n",
        "    total_size = len(data)\n",
        "    train_size = int(total_size * train_split)\n",
        "    val_size = int(total_size * val_split)\n",
        "    test_size = total_size - train_size - val_size\n",
        "    train_data, remaining_data = random_split(data, [train_size, total_size - train_size])\n",
        "    val_data, test_data = random_split(remaining_data, [val_size, test_size])\n",
        "    return list(train_data), list(val_data), list(test_data)\n",
        "\n",
        "# Custom Dataset class\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_raw_texts(self):\n",
        "        return [(src, trg) for src, trg in self.data]\n",
        "\n",
        "# Use basic English tokenizer\n",
        "tokenizer_en = get_tokenizer('basic_english')\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocabulary(tokenizer, dataset, min_freq=2):\n",
        "    def yield_tokens(data):\n",
        "        for src, tgt in data:\n",
        "            yield tokenizer(src)\n",
        "            yield tokenizer(tgt)\n",
        "\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset.get_raw_texts()),\n",
        "        specials=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"],\n",
        "        min_freq=min_freq\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "# Constants\n",
        "MAX_PADDING = 300\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Read and split data\n",
        "file_path = \"/content/drive/MyDrive/dataset.json\"\n",
        "raw_data = read_qa_data(file_path)\n",
        "train_data_raw, val_data_raw, test_data_raw = split_dataset(raw_data)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = QADataset(train_data_raw)\n",
        "valid_dataset = QADataset(val_data_raw)\n",
        "test_dataset = QADataset(test_data_raw)\n",
        "\n",
        "# Build vocab\n",
        "vocab = build_vocabulary(tokenizer_en, train_dataset)\n",
        "import pickle\n",
        "\n",
        "# Save the vocab to your Google Drive\n",
        "with open('/content/drive/MyDrive/vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "\n",
        "\n",
        "PAD_IDX = vocab['<pad>']\n",
        "SOS_IDX = vocab['<sos>']\n",
        "EOS_IDX = vocab['<eos>']\n",
        "\n",
        "# Batch generation function\n",
        "def generate_batch(data_batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for (src_text, tgt_text) in data_batch:\n",
        "        src_indices = torch.tensor([vocab[token] for token in tokenizer_en(src_text)], dtype=torch.long)\n",
        "        tgt_indices = torch.tensor([vocab[token] for token in tokenizer_en(tgt_text)], dtype=torch.long)\n",
        "\n",
        "        src_tensor = torch.cat([torch.tensor([SOS_IDX]), src_indices, torch.tensor([EOS_IDX])]).to(device)\n",
        "        tgt_tensor = torch.cat([torch.tensor([SOS_IDX]), tgt_indices, torch.tensor([EOS_IDX])]).to(device)\n",
        "\n",
        "        src_padded = F.pad(src_tensor, (0, MAX_PADDING - len(src_tensor)), value=PAD_IDX)\n",
        "        tgt_padded = F.pad(tgt_tensor, (0, MAX_PADDING - len(tgt_tensor)), value=PAD_IDX)\n",
        "\n",
        "        src_batch.append(src_padded)\n",
        "        tgt_batch.append(tgt_padded)\n",
        "\n",
        "    return torch.stack(src_batch), torch.stack(tgt_batch)\n",
        "\n",
        "# DataLoader setup\n",
        "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "print(len(vocab))\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          vocab_size:    size of vocabulary\n",
        "          d_model:       dimension of embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.lut = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x:        input tensor (batch_size, seq_length)\n",
        "\n",
        "        Returns:  embedding vector\n",
        "        \"\"\"\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model:     dimension of embeddings\n",
        "            dropout:     dropout probability\n",
        "            max_length:  max sequence length for positional encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "        for pos in range(max_length):\n",
        "            for i in range(0, d_model // 2):\n",
        "                theta = pos / (100 ** ((2 * i) / d_model))\n",
        "                pe[pos, 2 * i] = math.sin(theta)\n",
        "                pe[pos, 2 * i + 1] = math.cos(theta)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: embeddings (batch_size, seq_length, d_model)\n",
        "        Returns:\n",
        "            embeddings + positional encodings (batch_size, seq_length, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(1)].unsqueeze(0).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model:      dimension of embeddings\n",
        "            n_heads:      number of attention heads\n",
        "            dropout:      dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_key = d_model // n_heads\n",
        "\n",
        "        self.Wq = nn.Linear(d_model, d_model)\n",
        "        self.Wk = nn.Linear(d_model, d_model)\n",
        "        self.Wv = nn.Linear(d_model, d_model)\n",
        "        self.Wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: (batch_size, q_len, d_model)\n",
        "            key:   (batch_size, k_len, d_model)\n",
        "            value: (batch_size, v_len, d_model)\n",
        "            mask:  optional mask (batch_size, 1, 1, k_len)  or (batch_size, 1, q_len, k_len)\n",
        "\n",
        "        Returns:\n",
        "            output:     (batch_size, q_len, d_model)\n",
        "            attn_probs: (batch_size, n_heads, q_len, k_len)\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        Q = self.Wq(query)  # (batch_size, q_len, d_model)\n",
        "        K = self.Wk(key)    # (batch_size, k_len, d_model)\n",
        "        V = self.Wv(value)  # (batch_size, v_len, d_model)\n",
        "\n",
        "        # Split heads\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.d_key).transpose(1, 2)  # (batch_size, n_heads, q_len, d_key)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.d_key).transpose(1, 2)  # (batch_size, n_heads, k_len, d_key)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.d_key).transpose(1, 2)  # (batch_size, n_heads, v_len, d_key)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_key)  # (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask shape must broadcast to (batch_size, n_heads, q_len, k_len)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = F.softmax(scores, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "\n",
        "        A = torch.matmul(attn_probs, V)  # (batch_size, n_heads, q_len, d_key)\n",
        "\n",
        "        A = A.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_key)  # (batch_size, q_len, d_model)\n",
        "        output = self.Wo(A)\n",
        "\n",
        "        return output, attn_probs\n",
        "\n",
        "\n",
        "def create_padding_mask(seq, pad_idx):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      seq: tensor (batch_size, seq_len)\n",
        "      pad_idx: padding token index\n",
        "\n",
        "    Returns:\n",
        "      mask: (batch_size, 1, 1, seq_len)\n",
        "    \"\"\"\n",
        "    mask = (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
        "    return mask\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: embedding dimension\n",
        "            d_ffn: hidden dimension of feed-forward layer\n",
        "            dropout: dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
        "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input tensor (batch_size, seq_length, d_model)\n",
        "\n",
        "        Returns:\n",
        "            transformed tensor (batch_size, seq_length, d_model)\n",
        "        \"\"\"\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "      def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float):\n",
        "          \"\"\"\n",
        "          Single encoder block for answer generation (same as translation)\n",
        "          \"\"\"\n",
        "          super().__init__()\n",
        "          self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "          self.attn_layer_norm = nn.LayerNorm(d_model)\n",
        "          self.positionwise_fnn = PositionwiseFeedForward(d_model, d_ffn, dropout)\n",
        "          self.fnn_layer_norm = nn.LayerNorm(d_model)\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, src, src_mask):\n",
        "          \"\"\"\n",
        "          src: (batch_size, seq_len, d_model)\n",
        "          src_mask: optional padding mask\n",
        "          \"\"\"\n",
        "          _src, attn_probs = self.attention(src, src, src, src_mask)\n",
        "          src = self.attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "          _src = self.positionwise_fnn(src)\n",
        "          src = self.fnn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "          return src, attn_probs\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "      def __init__(self, d_model: int, n_layers: int, n_heads: int, d_ffn: int, dropout: float = 0.1):\n",
        "          \"\"\"\n",
        "          Stack of EncoderLayers\n",
        "          \"\"\"\n",
        "          super().__init__()\n",
        "          self.layers = nn.ModuleList([\n",
        "              EncoderLayer(d_model, n_heads, d_ffn, dropout)\n",
        "              for _ in range(n_layers)\n",
        "          ])\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, src, src_mask):\n",
        "          \"\"\"\n",
        "          src: embedded input sequence (batch, seq_len, d_model)\n",
        "          src_mask: padding mask\n",
        "          \"\"\"\n",
        "          for layer in self.layers:\n",
        "              src, attn_probs = layer(src, src_mask)\n",
        "\n",
        "          self.attn_probs = attn_probs  # Save last attention map if needed\n",
        "          return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.masked_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.masked_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.positionwise_fnn = PositionwiseFeedForward(d_model, d_ffn, dropout)\n",
        "        self.fnn_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \"\"\"\n",
        "        trg: (batch, tgt_len, d_model) - decoder input\n",
        "        src: (batch, src_len, d_model) - encoder output\n",
        "        trg_mask: (batch, 1, tgt_len, tgt_len) - causal mask\n",
        "        src_mask: (batch, 1, 1, src_len) - padding mask\n",
        "        \"\"\"\n",
        "        _trg, attn_probs = self.masked_attention(trg, trg, trg, trg_mask)\n",
        "        trg = self.masked_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        _trg, attn_probs = self.attention(trg, src, src, src_mask)\n",
        "        trg = self.attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        _trg = self.positionwise_fnn(trg)\n",
        "        trg = self.fnn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        return trg, attn_probs\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_ffn: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ffn, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Wo = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \"\"\"\n",
        "        trg: embedded target tokens (batch, tgt_len, d_model)\n",
        "        src: encoder output (batch, src_len, d_model)\n",
        "        trg_mask: causal mask for target (batch, 1, tgt_len, tgt_len)\n",
        "        src_mask: padding mask for encoder output (batch, 1, 1, src_len)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            trg, attn_probs = layer(trg, src, trg_mask, src_mask)\n",
        "\n",
        "        self.attn_probs = attn_probs\n",
        "        return self.Wo(trg)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: Embeddings,\n",
        "                 trg_embed: Embeddings, src_pad_idx: int, trg_pad_idx: int, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            encoder:        encoder stack\n",
        "            decoder:        decoder stack\n",
        "            src_embed:      question/context embeddings\n",
        "            trg_embed:      answer embeddings\n",
        "            src_pad_idx:    padding index for input\n",
        "            trg_pad_idx:    padding index for output\n",
        "            device:         cpu or gpu\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.trg_embed = trg_embed\n",
        "        self.device = device\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        \"\"\"\n",
        "        Creates padding mask for encoder input\n",
        "        \"\"\"\n",
        "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        \"\"\"\n",
        "        Creates padding + causal mask for decoder input\n",
        "        \"\"\"\n",
        "        seq_length = trg.size(1)\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_sub_mask = torch.tril(torch.ones((seq_length, seq_length), device=self.device)).bool()\n",
        "        return trg_pad_mask & trg_sub_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        src: (batch_size, src_seq_length) - question + context\n",
        "        trg: (batch_size, trg_seq_length) - answer (shifted)\n",
        "        Returns:\n",
        "            logits: (batch_size, trg_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        memory = self.encoder(self.src_embed(src), src_mask)\n",
        "        output = self.decoder(self.trg_embed(trg), memory, trg_mask, src_mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_model(device, src_vocab, trg_vocab, n_layers: int = 3, d_model: int = 256,\n",
        "               d_ffn: int = 2048, n_heads: int = 8, dropout: float = 0.1,\n",
        "               max_length: int = 5000):\n",
        "    \"\"\"\n",
        "    Constructs a Transformer model for answer generation.\n",
        "\n",
        "    Args:\n",
        "        src_vocab: source vocabulary (e.g. for question + context)\n",
        "        trg_vocab: target vocabulary (e.g. for answer)\n",
        "        device: torch device\n",
        "        n_layers: number of encoder & decoder layers\n",
        "        d_model: embedding dimension\n",
        "        d_ffn: feed-forward hidden dimension\n",
        "        n_heads: number of attention heads\n",
        "        dropout: dropout probability\n",
        "        max_length: max sequence length for positional encoding\n",
        "\n",
        "    Returns:\n",
        "        A full Transformer model instance\n",
        "    \"\"\"\n",
        "    encoder = Encoder(d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "    decoder = Decoder(len(trg_vocab), d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "\n",
        "    src_embed = Embeddings(len(src_vocab), d_model)\n",
        "    trg_embed = Embeddings(len(trg_vocab), d_model)\n",
        "    pos_enc = PositionalEncoding(d_model, dropout, max_length)\n",
        "\n",
        "    model = Transformer(\n",
        "        encoder,\n",
        "        decoder,\n",
        "        nn.Sequential(src_embed, pos_enc),\n",
        "        nn.Sequential(trg_embed, pos_enc),\n",
        "        src_pad_idx=src_vocab.get_stoi().get(\"<pad>\", 0),\n",
        "        trg_pad_idx=trg_vocab.get_stoi().get(\"<pad>\", 0),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "import math\n",
        "# Set device for model (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Build vocab from source and target datasets\n",
        "# Assuming you have already built vocab_src and vocab_trg using the build_vocabulary function.\n",
        "# If not, you need to do that before calling make_model. Here is how to get them:\n",
        "# vocab_src = build_vocabulary(tokenizer_en, train_dataset)  # For question + context\n",
        "# vocab_trg = build_vocabulary(tokenizer_en, train_dataset)  # For answer (it might differ in some cases)\n",
        "\n",
        "# Create the model using make_model function\n",
        "model = make_model(device, vocab, vocab,  # Assuming vocab_src and vocab_trg are the same for now\n",
        "                   n_layers=3, n_heads=8, d_model=256,\n",
        "                   d_ffn=512, max_length=300)\n",
        "\n",
        "# Move model to the correct device (GPU or CPU)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN2zUC2GywLU",
        "outputId": "aca62d62-baf4-435e-ba8f-f8845ad6e6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6681\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (positionwise_fnn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (fnn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x DecoderLayer(\n",
              "        (masked_attention): MultiHeadAttention(\n",
              "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (masked_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): MultiHeadAttention(\n",
              "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (positionwise_fnn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (fnn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (Wo): Linear(in_features=256, out_features=6681, bias=True)\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(6681, 256)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (trg_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(6681, 256)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Loss function - CrossEntropyLoss with padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \"\"\"\n",
        "    Function to train the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: The Transformer model to be trained.\n",
        "        iterator: The data iterator (DataLoader) for training data.\n",
        "        optimizer: Optimizer (Adam in your case).\n",
        "        criterion: Loss function (CrossEntropyLoss).\n",
        "        clip: Gradient clipping value to avoid exploding gradients.\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: The average loss for the epoch.\n",
        "    \"\"\"\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Loop through each batch in the iterator\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # Get source (src) and target (trg) from batch\n",
        "        src, trg = batch\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model (excluding the last token in trg for input)\n",
        "        logits = model(src, trg[:, :-1])\n",
        "\n",
        "        # Prepare the expected output (excluding the first token in trg)\n",
        "        expected_output = trg[:, 1:]\n",
        "\n",
        "        # Calculate the loss (CrossEntropyLoss)\n",
        "        loss = criterion(logits.contiguous().view(-1, logits.shape[-1]),\n",
        "                         expected_output.contiguous().view(-1))\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to prevent gradient explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # Update the model's weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss for averaging later\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Return the average loss for the epoch\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Function to evaluate the model on the validation or test data.\n",
        "\n",
        "    Args:\n",
        "        model: The Transformer model to be evaluated.\n",
        "        iterator: The data iterator (DataLoader) for validation or test data.\n",
        "        criterion: Loss function (CrossEntropyLoss).\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: The average loss for the evaluation.\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Disable gradient computation for evaluation\n",
        "    with torch.no_grad():\n",
        "        # Loop through each batch in the iterator\n",
        "        for i, batch in enumerate(iterator):\n",
        "            # Get source (src) and target (trg) from the batch\n",
        "            src, trg = batch\n",
        "\n",
        "            # Forward pass through the model (excluding the last token in trg for input)\n",
        "            logits = model(src, trg[:, :-1])\n",
        "\n",
        "            # Prepare the expected output (excluding the first token in trg)\n",
        "            expected_output = trg[:, 1:]\n",
        "\n",
        "            # Calculate the loss (CrossEntropyLoss)\n",
        "            loss = criterion(logits.contiguous().view(-1, logits.shape[-1]),\n",
        "                             expected_output.contiguous().view(-1))\n",
        "\n",
        "            # Accumulate the loss for averaging later\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    # Return the average loss for the epoch\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Function to compute the elapsed time between start and end.\n",
        "\n",
        "    Args:\n",
        "        start_time: Start time (typically the time before starting the epoch).\n",
        "        end_time: End time (typically the time after finishing the epoch).\n",
        "\n",
        "    Returns:\n",
        "        elapsed_mins: Elapsed time in minutes.\n",
        "        elapsed_secs: Elapsed time in seconds.\n",
        "    \"\"\"\n",
        "    # Calculate the total elapsed time\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Convert the elapsed time into minutes and seconds\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "import time\n",
        "import torch\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "SAVE_PATH = \"/content/drive/MyDrive/transformer_checkpoint.pt\"  # where to save model\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'valid_loss': valid_loss,\n",
        "            'epoch': epoch,\n",
        "        }, SAVE_PATH)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")\n",
        "\n",
        " # Load best checkpoint\n",
        "checkpoint = torch.load('/content/drive/MyDrive/transformer_checkpoint.pt', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/transformer1_model.pth')\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/transformer1_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def generate_answer(model, src_sentence, src_vocab, trg_vocab, tokenizer, max_len=300, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generate an answer from a given input sentence using the trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Transformer model.\n",
        "        src_sentence: Source sentence (question + context) as a string.\n",
        "        src_vocab: Vocabulary object for source side.\n",
        "        trg_vocab: Vocabulary object for target side.\n",
        "        tokenizer: Tokenizer function to split sentence into tokens.\n",
        "        max_len: Maximum length of generated output.\n",
        "        device: CPU or GPU.\n",
        "\n",
        "    Returns:\n",
        "        generated_answer: The generated answer as a string.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and numericalize\n",
        "    tokens = tokenizer(src_sentence.lower())\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']  # adding special tokens if needed\n",
        "\n",
        "    src_indexes = [src_vocab.get_stoi().get(token, src_vocab.get_stoi()['<unk>']) for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)  # shape (1, src_len)\n",
        "\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    # Encode the source\n",
        "    with torch.no_grad():\n",
        "        memory = model.encoder(model.src_embed(src_tensor), src_mask)\n",
        "\n",
        "    # Start decoding\n",
        "    trg_indexes = [trg_vocab.get_stoi()['<sos>']]  # start with <sos>\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)  # shape (1, len_so_far)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(model.trg_embed(trg_tensor), memory, trg_mask, src_mask)\n",
        "\n",
        "        pred_token = output.argmax(-1)[:, -1].item()  # last token prediction\n",
        "\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_vocab.get_stoi()['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Convert generated indexes back to tokens\n",
        "    trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n",
        "\n",
        "    # Remove special tokens\n",
        "    generated_answer = trg_tokens[1:-1]  # skip <sos> and <eos>\n",
        "\n",
        "    return ' '.join(generated_answer)\n",
        "\n",
        "# Example usage\n",
        "src_sentence = \"how do i activate my card?\"\n",
        "print(src_sentence)\n",
        "generated = generate_answer(model, src_sentence, vocab, vocab, tokenizer_en, device=device)\n",
        "print(\"Generated Answer:\", generated)\n",
        "\n",
        "\n",
        "src_sentence = \"how to apply for a loan?\"\n",
        "print(src_sentence)\n",
        "generated = generate_answer(model, src_sentence, vocab, vocab, tokenizer_en, device=device)\n",
        "print(\"Generated Answer:\", generated)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5E0LG6bzio-",
        "outputId": "8c8841c3-f7e2-4f3e-9558-72e551d9ed3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 3m 15s\n",
            "\tTrain Loss: 3.129 | Train PPL:  22.860\n",
            "\tValid Loss: 1.714 | Valid PPL:   5.553\n",
            "Epoch: 02 | Time: 3m 18s\n",
            "\tTrain Loss: 1.552 | Train PPL:   4.721\n",
            "\tValid Loss: 1.284 | Valid PPL:   3.612\n",
            "Epoch: 03 | Time: 3m 18s\n",
            "\tTrain Loss: 1.276 | Train PPL:   3.584\n",
            "\tValid Loss: 1.126 | Valid PPL:   3.082\n",
            "Epoch: 04 | Time: 3m 18s\n",
            "\tTrain Loss: 1.145 | Train PPL:   3.142\n",
            "\tValid Loss: 1.042 | Valid PPL:   2.836\n",
            "Epoch: 05 | Time: 3m 18s\n",
            "\tTrain Loss: 1.063 | Train PPL:   2.894\n",
            "\tValid Loss: 0.987 | Valid PPL:   2.682\n",
            "Epoch: 06 | Time: 3m 18s\n",
            "\tTrain Loss: 1.004 | Train PPL:   2.729\n",
            "\tValid Loss: 0.950 | Valid PPL:   2.586\n",
            "Epoch: 07 | Time: 3m 18s\n",
            "\tTrain Loss: 0.959 | Train PPL:   2.609\n",
            "\tValid Loss: 0.925 | Valid PPL:   2.521\n",
            "Epoch: 08 | Time: 3m 18s\n",
            "\tTrain Loss: 0.923 | Train PPL:   2.517\n",
            "\tValid Loss: 0.903 | Valid PPL:   2.468\n",
            "Epoch: 09 | Time: 3m 18s\n",
            "\tTrain Loss: 0.894 | Train PPL:   2.444\n",
            "\tValid Loss: 0.886 | Valid PPL:   2.425\n",
            "Epoch: 10 | Time: 3m 18s\n",
            "\tTrain Loss: 0.868 | Train PPL:   2.382\n",
            "\tValid Loss: 0.875 | Valid PPL:   2.398\n",
            "Test Loss: 0.875 | Test PPL:   2.398\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "how do i activate my card?\n",
            "Generated Answer: sure ! i ' m here to assist you with activating your card for international usage . to activate your card , please follow these steps 1 . contact our customer support team at {{customer support phone number}} or visit our website at {{company website url}} . 2 . provide them with your card details , such as the card number , expiration date , and cvv code . 3 . they will guide you through the activation process and ensure that your card is activated for international transactions . if you have any further questions or need additional assistance , feel free to let me know . i ' m here to help ! let me know if you need any more details .\n",
            "how to apply for a loan?\n",
            "Generated Answer: absolutely ! i ' m here to assist you with applying for a loan . applying for a loan can be a significant decision , and i ' ll guide you through the process . here ' s what you need to do 1 . gather your financial information this includes your income statements , identification documents , and any other relevant financial information . 2 . research lenders take some time to research different lenders and compare their interest rates , terms , and eligibility criteria . this will help you find the best fit for your needs . 3 . contact lenders or credit unions to discuss your needs and financial situation . they will provide you with the necessary forms and guide you through the application process . 4 . fill out the application form accurately and provide all the required information . make sure to double-check all the details before submitting it . 5 . wait for approval after submitting your application , the lender will review it and assess your eligibility . this may take some time , so be patient . 6 . once your application is approved , the lender will review it and assess your eligibility for the loan . they may request additional information or documentation . 7 . if your loan application is approved , you will receive a loan offer detailing the loan amount , interest rate , and repayment schedule . 8 . review the terms carefully and conditions before accepting the offer . 9 . if you agree , sign the loan agreement and sign the agreement . if you agree to the terms , sign the agreement . 10 . finally , sign the loan agreement and sign the agreement electronically or electronically or electronically\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "src_sentence = \"how to lock my card?\"\n",
        "print(src_sentence)\n",
        "generated = generate_answer(model, src_sentence, vocab, vocab, tokenizer_en, device=device)\n",
        "print(\"Generated Answer:\", generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OyUQWKFzmxB",
        "outputId": "b1f14511-a3a7-455c-ad81-1b17d1d576cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how to lock my card?\n",
            "Generated Answer: sure ! i ' m here to assist you with locking your card . to lock your card , please follow these steps 1 . log in to your online banking account or mobile banking app . 2 . navigate to the cards or account section . 3 . look for the option to lock or freeze your card . 4 . click on that option and follow the prompts to confirm the card lock . if you encounter any difficulties or have further questions , please don ' t hesitate to let me know . i ' m here to help ! let me know if you need any more details .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPgaajI37iMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}